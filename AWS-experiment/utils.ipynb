{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model.logistic import safe_sparse_dot\n",
    "from sklearn.utils.extmath import squared_norm, log_logistic\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    plt = None \n",
    "    pass\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from PIL import Image\n",
    "from munkres import Munkres\n",
    "import os\n",
    "import errno\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "SCALE_TYPE = \"uniform\"\n",
    "bounds = None\n",
    "\n",
    "\n",
    "class DummyScaler:\n",
    "    def fit_transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return X\n",
    "\n",
    "\n",
    "def prepare_data(name, onehot=True, labelEncode=True):\n",
    "    if name == \"iris\" or name == \"irisQB\":\n",
    "        X, y = prepare_iris()\n",
    "    elif name == \"iris_b\":\n",
    "        X, y = prepare_iris(binary=True)\n",
    "    elif name == \"diabetes\" or name == \"diabetesQB\":\n",
    "        X, y = prepare_diabetes()\n",
    "    elif name == \"mushrooms\":\n",
    "        X, y = prepare_mushrooms()\n",
    "    elif name == \"cancer\" or name == \"cancerQB\":\n",
    "        X, y = prepare_cancer()\n",
    "    elif name == \"adult\":\n",
    "        X, y = prepare_adult(target=\"race\", onehot=onehot)\n",
    "    elif name == \"adult_b\" or name == \"adult10QB\" or name == \"adultQB\":\n",
    "        X, y = prepare_adult(target=\"income\", onehot=onehot)\n",
    "    elif name == \"steak\":\n",
    "        X, y = prepare_steak(onehot=onehot)\n",
    "    elif name == \"gss\":\n",
    "        X, y = prepare_gss(onehot=onehot)\n",
    "    elif name == \"moons\":\n",
    "        X, y = prepare_moons()\n",
    "    elif name == \"circles\":\n",
    "        X, y = prepare_circles()\n",
    "    elif name == \"circlesQB\":\n",
    "        X, y = prepare_circlesQB()\n",
    "    elif name == \"blobs\":\n",
    "        X, y = prepare_blobs()\n",
    "    elif name == \"class5\":\n",
    "        X, y = prepare_classification(num_classes=5)\n",
    "    elif name == \"digits\" or name == \"digits2\":\n",
    "        X, y = prepare_digits()\n",
    "    elif name == \"digits40\":\n",
    "        X, y = prepare_digits()\n",
    "        X, y = X.values[0:40, :], y.values[0:40]\n",
    "    elif name == \"digits_all\":\n",
    "        X, y = prepare_digits_all()\n",
    "    elif name == \"faces\":\n",
    "        return prepare_faces()\n",
    "    elif name == \"att_faces\":\n",
    "        return prepare_att_faces()\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset %s', name)\n",
    "\n",
    "    if SCALE_TYPE in [\"uniform\", \"uniform_int\", \"norm\"]:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    else:\n",
    "        scaler = DummyScaler()\n",
    "\n",
    "    if not onehot:\n",
    "        X = X.values\n",
    "        for i in range(X.shape[1]):\n",
    "            try:\n",
    "                X[:, i] = scaler.fit_transform(X[:, i])\n",
    "            except ValueError:\n",
    "                if labelEncode:\n",
    "                    X[:, i] = LabelEncoder().fit_transform(X[:, i])\n",
    "\n",
    "        X = pd.DataFrame(X)\n",
    "    else:\n",
    "        X = pd.DataFrame(scaler.fit_transform(X))\n",
    "    y = pd.Series(LabelEncoder().fit_transform(y))\n",
    "\n",
    "    if SCALE_TYPE == \"data\":\n",
    "        global bounds\n",
    "        bounds = [X.min(axis=0), X.max(axis=0)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "    #X_train, y_train = X, y\n",
    "    #X_test, y_test = X.copy(), y.copy()\n",
    "\n",
    "    return X_train, y_train, X, y, scaler\n",
    "\n",
    "\n",
    "def prepare_att_faces():\n",
    "    c = 0\n",
    "    X, y = [], []\n",
    "    for dirname, dirnames, filenames in os.walk('../data/att_faces'):\n",
    "        for subdirname in dirnames:\n",
    "            subject_path = os.path.join(dirname, subdirname)\n",
    "            for filename in os.listdir(subject_path):\n",
    "                im = Image.open(os.path.join(subject_path, filename))\n",
    "                im = im.convert(\"L\")\n",
    "                X.append(np.asarray(im, dtype=np.uint8).flatten())\n",
    "                y.append(c)\n",
    "            c = c + 1\n",
    "    return None, None, np.array(X), np.array(y), None \n",
    "\n",
    "def prepare_faces():\n",
    "    data = sklearn.datasets.fetch_olivetti_faces('../data', shuffle=False)\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    X = np.split(X, 40)\n",
    "    y = np.split(y, 40)\n",
    "\n",
    "    X_train = [x[0:7, :] for x in X]\n",
    "    X_test = [x[7:, :] for x in X]\n",
    "    y_train = [a[0:7] for a in y]\n",
    "    y_test = [a[7:] for a in y]\n",
    "    X_train = np.concatenate(X_train)\n",
    "    X_test = np.concatenate(X_test)\n",
    "    y_train = pd.Series(np.concatenate(y_train))\n",
    "    y_test = pd.Series(np.concatenate(y_test))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "def prepare_adult(target=\"income\", onehot=True):\n",
    "    data = pd.read_csv('../data/adult.csv', sep=r'\\s*,\\s*', engine='python')\n",
    "\n",
    "    cols = list(data.columns.values)\n",
    "    cols.remove(target)\n",
    "    \n",
    "\n",
    "    X = data[cols]\n",
    "    y = data[target]\n",
    "\n",
    "    if onehot:\n",
    "        X = pd.get_dummies(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_steak(onehot=True):\n",
    "    data = pd.read_csv('../data/steak.csv').dropna()\n",
    "    target = \"How do you like your steak prepared?\"\n",
    "\n",
    "    del data['RespondentID']\n",
    "\n",
    "    X = data[list(set(data.columns) - set([target]))]\n",
    "    y = data[target]\n",
    "\n",
    "    if onehot:\n",
    "        X = pd.get_dummies(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_gss(onehot=True):\n",
    "    data = pd.read_csv('../data/GSShappiness.csv')\n",
    "\n",
    "    del data['year']\n",
    "    del data['id']\n",
    "\n",
    "    data = data.dropna()\n",
    "    target = \"Happiness level\"\n",
    "\n",
    "    X = data[list(set(data.columns) - set([target]))]\n",
    "    y = data[target]\n",
    "\n",
    "    if onehot:\n",
    "        X = pd.get_dummies(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_iris(binary=False):\n",
    "    data = pd.read_csv('../data/iris.csv')\n",
    "\n",
    "    if binary:\n",
    "        data = data[data[' class'] != 'Iris-setosa']\n",
    "\n",
    "    data = data.iloc[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    X = data[data.columns[:-1]]\n",
    "    y = data[data.columns[-1]]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_blobs():\n",
    "    X, y = sklearn.datasets.make_blobs(5000, cluster_std=0.5)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_diabetes():\n",
    "    X, y = sklearn.datasets.load_svmlight_file(\n",
    "        '../binary-classifiers/targets/diabetes/diabetes')\n",
    "    return X.toarray(), y\n",
    "\n",
    "\n",
    "def prepare_mushrooms():\n",
    "    X, y = sklearn.datasets.load_svmlight_file(\n",
    "        '../binary-classifiers/targets/mushrooms/mushrooms')\n",
    "    return X.toarray(), y\n",
    "\n",
    "\n",
    "def prepare_cancer():\n",
    "    X, y = sklearn.datasets.load_svmlight_file(\n",
    "        '../binary-classifiers/targets/breast-cancer/train.scale')\n",
    "    return X.toarray(), y\n",
    "\n",
    "\n",
    "def prepare_circlesQB():\n",
    "    X, y = sklearn.datasets.load_svmlight_file(\n",
    "        '../binary-classifiers/targets/circle/test.scale')\n",
    "    return pd.DataFrame(X.toarray()), pd.Series(y)\n",
    "\n",
    "\n",
    "def prepare_circles():\n",
    "    X, y = sklearn.datasets.make_circles(5000, factor=.5, noise=.05)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_moons():\n",
    "    X, y = sklearn.datasets.make_moons(5000, noise=0.25)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_digits():\n",
    "    digits = sklearn.datasets.load_digits()\n",
    "    X = pd.DataFrame(digits.data)\n",
    "    y = pd.Series(digits.target)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_digits_all():\n",
    "    data = sklearn.datasets.fetch_mldata('MNIST original', data_home='../data')\n",
    "    X = data.data.astype(float)\n",
    "    y = np.asarray(data.target, dtype=int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_classification(num_classes=5):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=1000,\n",
    "                                                n_classes=num_classes,\n",
    "                                                n_informative=4)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def gen_query_set(n, test_size=100000, dtype=SCALE_TYPE):\n",
    "    \"\"\"\n",
    "    Produce a random vector of size n with values in the range [low,high)\n",
    "    \"\"\"\n",
    "    print(\"Scale_type: \",dtype)\n",
    "    if dtype == \"uniform\":\n",
    "        return np.random.uniform(-1, 1, size=(test_size, n))\n",
    "    elif dtype == \"uniform_int\":\n",
    "        return 2 * np.random.randint(2, size=(test_size, n)) - 1\n",
    "    elif dtype == \"norm\":\n",
    "        return np.random.randn(test_size, n)\n",
    "    elif dtype == \"data\":\n",
    "        min_x, max_x = bounds\n",
    "        data = np.zeros((test_size, n))\n",
    "        for i in range(n):\n",
    "            data[:, i] = np.random.uniform(min_x[i], max_x[i], size=test_size)\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data type\")\n",
    "\n",
    "\n",
    "def stat_distance(p, q):\n",
    "    assert p.shape == q.shape\n",
    "\n",
    "    tot = 0.5 * np.abs((p-q)).sum()\n",
    "\n",
    "    return tot/len(p)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y, bounds, filename=None):\n",
    "    if plt is None:\n",
    "        return\n",
    "\n",
    "    fig = plt.figure()\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compare_boundaries(pred_func1, pred_func2, bounds, filename=None):\n",
    "    if plt is None:\n",
    "        return\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z1 = pred_func1(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z1 = Z1.reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    # Plot the contour and training examples\n",
    "    plt.contour(xx, yy, Z1, cmap=plt.cm.Reds)\n",
    "\n",
    "    Z2 = pred_func2(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z2 = Z2.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contour(xx, yy, Z2, cmap=plt.cm.Blues)\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def line_search(X, Y, num_samples, predict_func, eps=1e-1):\n",
    "    # random points\n",
    "    idx1 = np.random.choice(range(len(X)), num_samples)\n",
    "\n",
    "    # random points from a different class\n",
    "    idx2 = np.zeros(num_samples, dtype=int)\n",
    "    for i in range(num_samples):\n",
    "        idx2[i] = np.random.choice(np.where(Y != Y[idx1[i]])[0])\n",
    "\n",
    "    return _line_search(X, Y, idx1, idx2, predict_func, eps)\n",
    "\n",
    "\n",
    "def _line_search(X, Y, idx1, idx2, predict_func, eps, append=False):\n",
    "    v1 = X[idx1, :]\n",
    "    y1 = Y[idx1]\n",
    "    v2 = X[idx2, :]\n",
    "    y2 = Y[idx2]\n",
    "\n",
    "    assert np.all(y1 != y2)\n",
    "\n",
    "    if append:\n",
    "        samples = X\n",
    "\n",
    "    # process all points in parallel\n",
    "    while np.any(np.sum((v1 - v2)**2, axis=-1)**(1./2) > eps):\n",
    "        # find all mid points\n",
    "        mid = 0.5 * (v1 + v2)\n",
    "\n",
    "        # query the class on the current model\n",
    "        y_mid = predict_func(mid)\n",
    "\n",
    "        # change either v1 or v2 depending on the value of y_mid\n",
    "        index1 = np.where(y_mid != y1)[0]\n",
    "        index2 = np.where(y_mid == y1)[0]\n",
    "\n",
    "        if len(index1):\n",
    "            v2[index1, :] = mid[index1, :]\n",
    "        if len(index2):\n",
    "            v1[index2, :] = mid[index2, :]\n",
    "\n",
    "        if append:\n",
    "            samples = np.vstack((samples, mid))\n",
    "\n",
    "    if append:\n",
    "        return samples\n",
    "    else:\n",
    "        return np.vstack((v1, v2))\n",
    "\n",
    "\n",
    "def all_pairs(Y):\n",
    "    classes = pd.Series(Y).unique().tolist()\n",
    "    return [(i, j)\n",
    "            for i in range(len(Y))              # go over all points\n",
    "            for c in classes                    # and all other classes\n",
    "            if c != Y[i]\n",
    "            for j in np.where(Y == c)[0][0:1]   # and build a pair\n",
    "            if i > j]\n",
    "\n",
    "\n",
    "def query_count(X, Y, eps):\n",
    "    dist = squareform(pdist(X, 'euclidean'))\n",
    "    tot = 0\n",
    "\n",
    "    for (i, j) in all_pairs(Y):\n",
    "        if dist[i][j] > eps:\n",
    "            tot += math.ceil(np.log2(dist[i][j]/eps))\n",
    "\n",
    "    return tot\n",
    "\n",
    "\n",
    "def line_search_oracle(n, budget, predict_func, query_gen, eps=1e-1):\n",
    "    X_init = query_gen(n, 1)\n",
    "    Y = predict_func(X_init)\n",
    "\n",
    "    tot_budget = budget\n",
    "    budget -= 1\n",
    "\n",
    "    step = (budget+3)/4\n",
    "\n",
    "    while query_count(X_init, Y, eps) <= budget:\n",
    "        x = query_gen(n, step)\n",
    "        y = predict_func(x)\n",
    "        X_init = np.vstack((X_init, x))\n",
    "        Y = np.hstack((Y, y))\n",
    "        budget -= step\n",
    "\n",
    "    if budget <= 0:\n",
    "        assert len(X_init) >= tot_budget\n",
    "        return X_init[0:tot_budget]\n",
    "\n",
    "    Y = Y.flatten()\n",
    "    idx1, idx2 = zip(*all_pairs(Y))\n",
    "    idx1 = list(idx1)\n",
    "    idx2 = list(idx2)\n",
    "    samples = _line_search(X_init, Y, idx1, idx2, predict_func, eps,\n",
    "                           append=True)\n",
    "\n",
    "    assert len(samples) >= tot_budget\n",
    "    return samples[0:tot_budget]\n",
    "\n",
    "\n",
    "def approx_fprime_helper(xk, f, epsilon, args=(), f0=None):\n",
    "    \"\"\"\n",
    "    See ``approx_fprime``.  An optional initial function value arg is added.\n",
    "    \"\"\"\n",
    "    if f0 is None:\n",
    "        f0 = f(*((xk,) + args))\n",
    "    grad = np.zeros(xk.shape, float)\n",
    "    ei = np.zeros(xk.shape, float)\n",
    "\n",
    "    for k in range(len(xk)):\n",
    "        ei[k] = 1.0\n",
    "        d = epsilon * ei\n",
    "        grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n",
    "        ei[k] = 0.0\n",
    "    return grad\n",
    "\n",
    "\n",
    "def temp_log_loss(w, X, Y, alpha):\n",
    "    n_classes = Y.shape[1]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    intercept = w[:, -1]\n",
    "    w = w[:, :-1]\n",
    "    z = safe_sparse_dot(X, w.T) + intercept\n",
    "\n",
    "    denom = expit(z)\n",
    "    #print denom\n",
    "    #print denom.sum()\n",
    "    denom = denom.sum(axis=1).reshape((denom.shape[0], -1))\n",
    "    #print denom\n",
    "    p = log_logistic(z)\n",
    "\n",
    "    loss = - (Y * p).sum()\n",
    "    loss += np.log(denom).sum()\n",
    "    loss += 0.5 * alpha * squared_norm(w)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def min_l1_dist(m1, m2):\n",
    "    assert len(m1) == len(m2)\n",
    "\n",
    "    # pairwise l1 distances\n",
    "    dist = cdist(m1, m2, 'minkowski', 1)\n",
    "\n",
    "    m = Munkres()\n",
    "    matching = m.compute(dist.copy())\n",
    "\n",
    "    total = 0.0\n",
    "    for row, column in matching:\n",
    "        value = dist[row][column]\n",
    "        total += value\n",
    "\n",
    "    return total, matching\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
