{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice850311/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model.logistic import _multinomial_loss, \\\n",
    "    _multinomial_loss_grad, safe_sparse_dot\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.extmath import squared_norm, log_logistic\n",
    "from scipy.special import expit, logit\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "%run utils.ipynb\n",
    "import warnings\n",
    "import timeit\n",
    "import sys\n",
    "import decimal\n",
    "\n",
    "\n",
    "def softmax(X, copy=True):\n",
    "    \"\"\"\n",
    "    Calculate the softmax function.\n",
    "    The softmax function is calculated by\n",
    "    np.exp(X) / np.sum(np.exp(X), axis=1)\n",
    "    This will cause overflow when large values are exponentiated.\n",
    "    Hence the largest value in each row is subtracted from each data\n",
    "    point to prevent this.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like, shape (M, N)\n",
    "        Argument to the logistic function\n",
    "    copy: bool, optional\n",
    "        Copy X or not.\n",
    "    Returns\n",
    "    -------\n",
    "    out: array, shape (M, N)\n",
    "        Softmax function evaluated at every point in x\n",
    "    \"\"\"\n",
    "    if copy:\n",
    "        X = np.copy(X)\n",
    "    \n",
    "    max_prob = np.max(X, axis=1).reshape((-1, 1))\n",
    "    \n",
    "    #print(\"max_prob \",max_prob)\n",
    "\n",
    "    X -= max_prob\n",
    "\n",
    "    np.exp(X, X)\n",
    "\n",
    "    sum_prob = np.sum(X, axis=1).reshape((-1, 1))\n",
    "    #print(\"sum_prob \",sum_prob)\n",
    "    X /= sum_prob\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_probas(X, w, intercept, multinomial=True):\n",
    "    \"\"\"\n",
    "    Predict probabilities for each class using either a multinomial or a\n",
    "    one-vs-rest approach\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"X.shape： \", X.shape)\n",
    "    #print(\"w.shape： \", w.shape)\n",
    "    #print (\"intercept.shape:　\", intercept.shape)\n",
    "\n",
    "    p = safe_sparse_dot(X, w.T, dense_output=True) + intercept\n",
    "    \n",
    "    #print(\"p size : \", p.shape)\n",
    "    \n",
    "    if multinomial:\n",
    "               \n",
    "        return softmax(p, copy=False)\n",
    "    else:\n",
    "        p = p.ravel() if p.shape[1] == 1 else p\n",
    "\n",
    "        p *= -1\n",
    "        np.exp(p, p)\n",
    "        p += 1\n",
    "        np.reciprocal(p, p)\n",
    "\n",
    "        if p.ndim == 1:\n",
    "            return np.vstack([1 - p, p]).T\n",
    "        else:\n",
    "            # OvR normalization, like LibLinear's predict_probability\n",
    "            p /= p.sum(axis=1).reshape((p.shape[0], -1))\n",
    "            return p\n",
    "\n",
    "\n",
    "def score_function(X, w, intercept):\n",
    "    \"\"\"\n",
    "    Score function to predict classes\n",
    "    \"\"\"\n",
    "    scores = safe_sparse_dot(X, w.T, dense_output=True) + intercept\n",
    "    return scores.ravel() if scores.shape[1] == 1 else scores\n",
    "\n",
    "\n",
    "def predict_classes(X, w, intercept, classes):\n",
    "    \"\"\"\n",
    "    Predict class labels for samples in X.\n",
    "    \"\"\"\n",
    "    scores = score_function(X, w, intercept)\n",
    "\n",
    "    if len(scores.shape) == 1:\n",
    "        indices = (scores > 0).astype(np.int)\n",
    "    else:\n",
    "        indices = scores.argmax(axis=1)\n",
    "\n",
    "    return classes[indices]\n",
    "\n",
    "\n",
    "def multinomial_loss(w, X, Y, alpha):\n",
    "    \"\"\"\n",
    "    Wrapper for the multinomial loss function used in scikit\n",
    "    \"\"\"\n",
    "    weights = np.ones((len(X),))\n",
    "    return _multinomial_loss(w, X, Y, alpha, weights)[0]\n",
    "\n",
    "def multnomial_grad(w, X, Y, alpha):\n",
    "    \"\"\"\n",
    "    Wrapper for the multinomial gradient function used in scikit\n",
    "    \"\"\"\n",
    "    weights = np.ones((len(X),))\n",
    "    \n",
    "    gg = _multinomial_loss_grad(w, X, Y, alpha, weights)[1]\n",
    "    #print(\"grad :\",gg)\n",
    "    return gg\n",
    "\n",
    "\n",
    "def logistic_loss(w, X, Y, alpha):\n",
    "    \"\"\"\n",
    "    Implementation of the logistic loss function when Y is a probability\n",
    "    distribution.\n",
    "\n",
    "    loss = -SUM_i SUM_k y_ik * log(P[yi == k]) + alpha * ||w||^2\n",
    "    \"\"\"\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    intercept = 0\n",
    "\n",
    "    if n_classes > 2:\n",
    "        fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "        w = w.reshape(n_classes, -1)\n",
    "        if fit_intercept:\n",
    "            intercept = w[:, -1]\n",
    "            w = w[:, :-1]\n",
    "    else:\n",
    "        fit_intercept = w.size == (n_features + 1)\n",
    "        if fit_intercept:\n",
    "            intercept = w[-1]\n",
    "            w = w[:-1]\n",
    "\n",
    "    z = safe_sparse_dot(X, w.T) + intercept\n",
    "\n",
    "    if n_classes == 2:\n",
    "        # in the binary case, simply compute the logistic function\n",
    "        p = np.vstack([log_logistic(-z), log_logistic(z)]).T\n",
    "    else:\n",
    "        # compute the logistic function for each class and normalize\n",
    "        denom = expit(z)\n",
    "        denom = denom.sum(axis=1).reshape((denom.shape[0], -1))\n",
    "        p = log_logistic(z)\n",
    "        loss = - (Y * p).sum()\n",
    "        loss += np.log(denom).sum()  # Y.sum() = 1\n",
    "        loss += 0.5 * alpha * squared_norm(w)\n",
    "        return loss\n",
    "\n",
    "    loss = - (Y * p).sum() + 0.5 * alpha * squared_norm(w)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def logistic_grad_bin(w, X, Y, alpha):\n",
    "    \"\"\"\n",
    "    Implementation of the logistic loss gradient when Y is a binary probability\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    grad = np.empty_like(w)\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    fit_intercept = w.size == (n_features + 1)\n",
    "\n",
    "    if fit_intercept:\n",
    "        intercept = w[-1]\n",
    "        w = w[:-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "\n",
    "    z = safe_sparse_dot(X, w.T) + intercept\n",
    "\n",
    "    _, n_features = X.shape\n",
    "    z0 = - (Y[:, 1] + (expit(-z) - 1))\n",
    "\n",
    "    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
    "\n",
    "    if fit_intercept:\n",
    "        grad[-1] = z0.sum()\n",
    "\n",
    "    return grad.flatten()\n",
    "    \n",
    "    \n",
    "def logistic_grad(w, X, Y, alpha):\n",
    "    \"\"\"\n",
    "    Implementation of the logistic loss gradient when Y is a multi-ary\n",
    "    probability distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    grad = np.zeros((n_classes, n_features + int(fit_intercept)))\n",
    "\n",
    "    w = w.reshape(n_classes, -1)\n",
    "\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "\n",
    "    z = safe_sparse_dot(X, w.T) + intercept\n",
    "\n",
    "    # normalization factor\n",
    "    denom = expit(z)\n",
    "    denom = denom.sum(axis=1).reshape((denom.shape[0], -1))\n",
    "\n",
    "    #\n",
    "    # d/dwj log(denom)\n",
    "    #       = 1/denom * d/dw expit(wj * x + b)\n",
    "    #       = 1/denom * expit(wj * x + b) * expit(-(wj * x + b)) * x\n",
    "    #\n",
    "    # d/dwj -Y * log_logistic(z)\n",
    "    #       = -Y * expit(-(wj * x + b)) * x\n",
    "    #\n",
    "    z0 = (np.reciprocal(denom) * expit(z) - Y) * expit(-z)\n",
    "    \n",
    "    grad[:, :n_features] = safe_sparse_dot(z0.T, X)\n",
    "    grad[:, :n_features] += alpha * w\n",
    "\n",
    "    if fit_intercept:\n",
    "        grad[:, -1] = z0.sum(axis=0)\n",
    "\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "class RegressionExtractor(object):\n",
    "    \"\"\"\n",
    "    Extract coefficients of a logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classes = self.get_classes()\n",
    "        self.X_train = None\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def num_features(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_classes(self):\n",
    "        return\n",
    "\n",
    "    def gen_query_set(self, n, test_size, force_input_space=True):\n",
    "        return gen_query_set(n, test_size)\n",
    "\n",
    "    def run_opti(self, loss, grad, X, Y, w_dim):\n",
    "        \"\"\"\n",
    "        Wrapper for the optimization procedure.\n",
    "        Try different regularizers, optimization strategies and random starting\n",
    "        points until we achieve an overwhelming accuracy on the training set\n",
    "        \"\"\"\n",
    "        k = Y.shape[1]\n",
    "\n",
    "        best_w = None\n",
    "        best_int = None\n",
    "        best_acc = 0\n",
    "\n",
    "        maxiter = 1\n",
    "        alphas = [10**x for x in range(-16, -8)]\n",
    "        fprimes = [grad]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        #print(\"len: \",len(fprimes))\n",
    "        for fprime in fprimes:\n",
    "\n",
    "            for alpha in alphas:\n",
    "                for i in range(maxiter):\n",
    "\n",
    "                    \"\"\"\n",
    "                    w_true = np.zeros(w_dim)\n",
    "                    w_true[:, :-1] = self.w\n",
    "                    w_true[:, -1] = self.intercept\n",
    "                    print loss(w_true, X, Y, alpha)\n",
    "                    \"\"\"\n",
    "\n",
    "                    w0 = 1e-8 * np.random.randn(*w_dim)\n",
    "                    print(\"w0.shape\",w0.shape)\n",
    "                    \"\"\"\n",
    "                    print loss(w0, X, Y, alpha)\n",
    "                    print logistic_grad(w0, X, Y, alpha)\n",
    "                    print utils.approx_fprime_helper(w0.ravel(), loss, 1e-8,\n",
    "                                                     args=(X, Y, alpha))\n",
    "                    \"\"\"\n",
    "\n",
    "                    num_unknowns = len(w0.ravel())\n",
    "                    method = \"BFGS\"\n",
    "                    if num_unknowns >= 1000:\n",
    "                        method = \"L-BFGS-B\"\n",
    "\n",
    "                    print ('finding solution of system of {} equations with {}' \\\n",
    "                          ' unknowns with {}'.format(len(X), num_unknowns, method))\n",
    "\n",
    "                    try:\n",
    "                        print(\"optimize logit BFGS\")\n",
    "                        optimLogitBFGS = minimize(loss, x0=w0,\n",
    "                                                  method=method,\n",
    "                                                  args=(X, Y, alpha),\n",
    "                                                  jac=fprime,\n",
    "                                                  options={'gtol': 1e-6,\n",
    "                                                           'disp': True,\n",
    "                                                           'maxiter': 100})\n",
    "                        wopt = optimLogitBFGS.x\n",
    "                    except ValueError:\n",
    "                        wopt = np.zeros(w0.shape)\n",
    "\n",
    "                    # reshape the coefficient vector\n",
    "                    if k == 2:\n",
    "                        int_opt = wopt[-1]\n",
    "                        wopt = np.array([wopt[:-1]])\n",
    "                    else:\n",
    "                        wopt = wopt.reshape(k, -1)\n",
    "                        int_opt = wopt[:, -1]\n",
    "                        wopt = wopt[:, :-1]\n",
    "\n",
    "                    # check the accuracy over the small set of training vectors\n",
    "                    #print(\"wopt size:\" , wopt.shape)\n",
    "                    #print(\"x size:\" , X.shape)\n",
    "                    acc = self.evaluate(wopt, int_opt, X)\n",
    "                    print ('obtained train accuracy of {}'.format(acc))\n",
    "                    if acc > 0.99:\n",
    "                        end_time = timeit.default_timer()\n",
    "                        print (\"opti ran for %.2f s\" \\\n",
    "                                             % (end_time - start_time))\n",
    "                        return wopt, int_opt\n",
    "                    if acc >= best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_w = wopt\n",
    "                        best_int = int_opt\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "        print ( \"opti ran for %.2f s\" % (end_time - start_time))\n",
    "        return best_w, best_int\n",
    "\n",
    "    def find_coeffs(self, m, baseline=False, adapt=False):\n",
    "        k = len(self.classes)       # number of classes\n",
    "        n = self.num_features()     # vector dimension\n",
    "\n",
    "        # generate random queries\n",
    "        if not adapt:\n",
    "            X = self.gen_query_set(n, test_size=m)\n",
    "        else:\n",
    "            X = utils.line_search_oracle(n, m, self.query, self.gen_query_set)\n",
    "\n",
    "        self.X_train = X\n",
    "\n",
    "        # get the probabilities for all queries\n",
    "        if baseline:\n",
    "            model = self.baseline_model(X)\n",
    "\n",
    "            return model\n",
    "        else:\n",
    "            Y = self.query_probas(X)\n",
    "\n",
    "        return self.select_and_run_opti(k, n, X, Y)\n",
    "\n",
    "    def select_and_run_opti(self, k, n, X, Y):\n",
    "\n",
    "        if self.multinomial:\n",
    "            \"\"\"\n",
    "            Recover the full coefficient vector by minimizing the\n",
    "            cross entropy loss.\n",
    "            \"\"\"\n",
    "            wdim = (k, n + 1)\n",
    "            #print(\"wdim: \",wdim)\n",
    "            wopt, int_opt = self.run_opti(multinomial_loss,\n",
    "                                          multnomial_grad, X, Y, wdim)\n",
    "        else:\n",
    "            if k == 2:\n",
    "                \"\"\"\n",
    "                Recover the single coefficient vector by minimizing the\n",
    "                cross entropy loss.\n",
    "                \"\"\"\n",
    "                wdim = (1, n + 1)\n",
    "                wopt, int_opt = self.run_opti(logistic_loss,\n",
    "                                              logistic_grad_bin, X, Y, wdim)\n",
    "            else:\n",
    "                \"\"\"\n",
    "                Recover the full coefficient vector by minimizing the\n",
    "                cross entropy loss.\n",
    "                \"\"\"\n",
    "                wdim = (k, n + 1)\n",
    "                wopt, int_opt = self.run_opti(logistic_loss,\n",
    "                                              logistic_grad, X, Y, wdim)\n",
    "\n",
    "        return wopt, int_opt, len(X)\n",
    "    \n",
    "    def find_coeffs_bin(self, budget):\n",
    "        k = len(self.classes)       # number of classes\n",
    "        assert k == 2\n",
    "        n = self.num_features()     # vector dimension\n",
    "        \n",
    "        X_train = self.gen_query_set(n, budget)\n",
    "        \n",
    "        #print(\"find coeff x: \",X_train.shape)\n",
    "        out = self.query_probas(X_train)[:, 0]\n",
    "        #print(\"out: \", out)\n",
    "        y = logit(out)\n",
    "        print(\"logit out shape: \",y.shape)\n",
    "        X = np.hstack((X_train, np.ones((budget, 1))))\n",
    "\n",
    "        if budget == n+1:\n",
    "            try:\n",
    "                w_opt = np.linalg.solve(X, y).T\n",
    "            except np.linalg.linalg.LinAlgError:\n",
    "                w_opt = np.linalg.lstsq(X, y)[0].T\n",
    "        else:\n",
    "            w_opt = np.linalg.lstsq(X, y)[0].T\n",
    "\n",
    "        int_opt = w_opt[-1]\n",
    "        w_opt = np.array([w_opt[:-1]])\n",
    "\n",
    "        self.X_train = X_train\n",
    "\n",
    "        return w_opt, int_opt\n",
    "\n",
    "    def find_coeffs_adaptive(self, step, query_budget, baseline=False):\n",
    "        assert query_budget > 0\n",
    "        k = len(self.classes)       # number of classes\n",
    "        n = self.num_features()     # vector dimension\n",
    "\n",
    "        X = self.gen_query_set(n, test_size=step)\n",
    "\n",
    "        while query_budget > 0:\n",
    "            query_budget -= step\n",
    "            # print 'training with {} queries'.format(len(X))\n",
    "            if baseline:\n",
    "                model = self.baseline_model(X)\n",
    "            else:\n",
    "                Y = self.query_probas(X)\n",
    "                w_opt, int_opt, _ = self.select_and_run_opti(k, n, X, Y)\n",
    "\n",
    "            if baseline:\n",
    "                predict_func = lambda x: model.predict(x)\n",
    "                predict_func_p = lambda x: model.predict_proba(x)\n",
    "            else:\n",
    "                predict_func = lambda x: predict_classes(x, w_opt, int_opt,\n",
    "                                                         self.get_classes())\n",
    "                predict_func_p = lambda x: predict_probas(x, w_opt, int_opt,\n",
    "                                                          self.multinomial)\n",
    "\n",
    "            if query_budget > 0:\n",
    "                X_local = self.gen_query_set(n, test_size=query_budget)\n",
    "                Y_local = predict_func(X_local)\n",
    "\n",
    "                if len(pd.Series(Y_local[0:100]).unique()) == 1 \\\n",
    "                        or callable(getattr(self, 'encode', None)):\n",
    "                    Y_local_p = predict_func_p(X_local)\n",
    "\n",
    "                    if Y_local_p.ndim == 1 or Y_local_p.shape[1] == 1:\n",
    "                        Y_local_p = np.hstack([1 - Y_local_p, Y_local_p])\n",
    "\n",
    "                    Y_local_p.sort()\n",
    "                    scores = Y_local_p[:, -1] - Y_local_p[:, -2]\n",
    "\n",
    "                    adaptive_budget = (min(step, query_budget)*3)/4\n",
    "                    random_budget = min(step, query_budget) - adaptive_budget\n",
    "\n",
    "                    indices = scores.argsort()[0:adaptive_budget]\n",
    "                    samples = X_local[indices, :]\n",
    "                    X_random = self.gen_query_set(n, random_budget)\n",
    "                    samples = np.vstack((samples, X_random))\n",
    "                else:\n",
    "                    # reserve some budget for random queries\n",
    "                    adaptive_budget = (min(step, query_budget)*3)/4\n",
    "                    adaptive_budget += adaptive_budget % 2\n",
    "                    random_budget = min(step, query_budget) - adaptive_budget\n",
    "                    samples = utils.line_search(X_local[0:100], Y_local[0:100],\n",
    "                                                adaptive_budget/2,\n",
    "                                                predict_func)\n",
    "                    X_random = self.gen_query_set(n, random_budget)\n",
    "                    samples = np.vstack((samples, X_random))\n",
    "\n",
    "                assert len(samples) == min(step, query_budget)\n",
    "\n",
    "                X = np.vstack((X, samples))\n",
    "\n",
    "        if baseline:\n",
    "            return model\n",
    "        else:\n",
    "            return w_opt, int_opt\n",
    "\n",
    "    def evaluate(self, wopt, int_opt, X_test, base_model=None):\n",
    "        # get the true class labels\n",
    "        y_true = self.query(X_test)\n",
    "        #print(\"y_true\", y_true)\n",
    "        if X_test.shape[1] != self.num_features():\n",
    "            X_test = self.encode(X_test)\n",
    "\n",
    "        # predict classes using the optimized coefficients\n",
    "        y_pred = predict_classes(X_test, wopt, int_opt, self.classes)\n",
    "        #print(\"y_pred\", y_pred)\n",
    "        \"\"\"\n",
    "        _, _, X, _, _ = utils.prepare_data(self.model_id, onehot=False)\n",
    "        X = X.values\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] != y_pred[i]:\n",
    "                print y_true[i], y_pred[i], X[i]\n",
    "        \"\"\"\n",
    "\n",
    "        if base_model is not None:\n",
    "\n",
    "            y_pred_base = base_model.predict(X_test)\n",
    "\n",
    "            return accuracy_score(y_true, y_pred), \\\n",
    "                   accuracy_score(y_true, y_pred_base)\n",
    "\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    def evaluate_probas(self, wopt, int_opt, X_test, base_model=None):\n",
    "        # get the true class probas\n",
    "        y_true = self.query_probas(X_test)\n",
    "\n",
    "        if X_test.shape[1] != self.num_features():\n",
    "            X_test = self.encode(X_test)\n",
    "\n",
    "        # predict class probas using the optimized coefficients\n",
    "        y_pred = predict_probas(X_test, wopt, int_opt, self.multinomial)\n",
    "\n",
    "        if base_model is not None:\n",
    "            y_pred_base = base_model.predict_proba(X_test)\n",
    "            y_pred_base = pd.DataFrame(y_pred_base, columns=base_model.classes_)\n",
    "\n",
    "            for col in self.classes:\n",
    "                if col not in base_model.classes_:\n",
    "                    y_pred_base[col] = 0\n",
    "\n",
    "            y_pred_base.columns = self.classes\n",
    "\n",
    "            return stat_distance(y_pred, y_true), \\\n",
    "                   stat_distance(y_pred_base.as_matrix(), y_true)\n",
    "\n",
    "        return stat_distance(y_pred, y_true)\n",
    "\n",
    "    def evaluate_model(self, wopt, int_opt, base_model=None):\n",
    "        try:\n",
    "            w_true = self.w\n",
    "            int_true = self.intercept\n",
    "\n",
    "            if self.multinomial:\n",
    "                w_true = w_true - w_true[0]\n",
    "                \n",
    "                wopt = wopt - wopt[0]\n",
    "          \n",
    "                int_true = int_true - int_true[0]\n",
    "\n",
    "                int_opt = int_opt \n",
    "                \n",
    "                \n",
    "            loss = np.sum(np.abs(w_true - wopt)) + \\\n",
    "                   np.sum(np.abs(int_true - int_opt))\n",
    "  \n",
    "            if base_model is not None:\n",
    "                if isinstance(base_model, DummyClassifier) \\\n",
    "                        or len(self.get_classes()) != len(base_model.classes_):\n",
    "                    loss_base = np.sum(np.abs(w_true)) + \\\n",
    "                                np.sum(np.abs(int_true))\n",
    "                else:\n",
    "                    w_base = base_model.coef_\n",
    "                    int_base = base_model.intercept_\n",
    "\n",
    "                    if self.multinomial:\n",
    "                        w_base = w_base - w_true[0]\n",
    "                        int_base = int_base - int_base[0]\n",
    "\n",
    "                    loss_base = np.sum(np.abs(w_true - w_base)) + \\\n",
    "                                np.sum(np.abs(int_true - int_base))\n",
    "\n",
    "                return loss, loss_base\n",
    "            return loss\n",
    "\n",
    "        except AttributeError:\n",
    "            return np.nan\n",
    "\n",
    "    def baseline_model(self, X):\n",
    "        Y = pd.Series(self.query(X))\n",
    "\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                if self.multinomial:\n",
    "                    model = LogisticRegression(solver='lbfgs',\n",
    "                                               C=1e40,\n",
    "                                               multi_class='multinomial',\n",
    "                                               tol=1e-20, max_iter=10000)\n",
    "                else:\n",
    "                    model = LogisticRegression(solver='lbfgs',\n",
    "                                               C=1e40,\n",
    "                                               multi_class='ovr', tol=1e-20,\n",
    "                                               max_iter=10000)\n",
    "\n",
    "                model.fit(X, Y)\n",
    "\n",
    "        except ValueError:\n",
    "            model = DummyClassifier(strategy=\"stratified\")\n",
    "            model.fit(X, Y)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def lowd_meek(self, budget, delta=1e-8):\n",
    "        [c1, c0] = self.get_classes()\n",
    "\n",
    "        n = self.num_features()\n",
    "        m = 0\n",
    "        x_pos = None\n",
    "        x_neg = None\n",
    "\n",
    "        #\n",
    "        # Find a positive and a negative instance\n",
    "        #\n",
    "        while x_pos is None or x_neg is None:\n",
    "            x = self.gen_query_set(n, 1)[0]\n",
    "            c = self.query([x])[0]\n",
    "            if c == c0:\n",
    "                x_pos = x\n",
    "            else:\n",
    "                x_neg = x\n",
    "            m += 1\n",
    "\n",
    "        s_pos = x_pos.copy()\n",
    "        s_neg = None\n",
    "        f = None\n",
    "        #\n",
    "        # Find a sign witness\n",
    "        #\n",
    "        for i in range(n):\n",
    "            s_pos_old = s_pos.copy()\n",
    "            s_pos[i] = x_neg[i]\n",
    "            m += 1\n",
    "\n",
    "            if self.query([s_pos])[0] == c1:\n",
    "                s_neg = s_pos.copy()\n",
    "                s_pos = s_pos_old\n",
    "                f = i\n",
    "                break\n",
    "\n",
    "        if m >= budget:\n",
    "            return np.zeros((1,n)), 0, m\n",
    "\n",
    "        curr_m = m\n",
    "\n",
    "        for eps in [10.0**x for x in range(-6, 1)]:\n",
    "            m = curr_m\n",
    "\n",
    "            w = np.zeros(n)\n",
    "            w[f] = (s_pos[f] - s_neg[f])/abs(s_pos[f] - s_neg[f])\n",
    "\n",
    "            x = s_pos.copy()\n",
    "            # binary search on feature f\n",
    "            while abs(s_pos[f] - s_neg[f]) >= eps/4:\n",
    "                x[f] = 0.5 * (s_pos[f] + s_neg[f])\n",
    "                c = self.query([x])\n",
    "                if c == c1:\n",
    "                    s_neg[f] = x[f]\n",
    "                else:\n",
    "                    s_pos[f] = x[f]\n",
    "                m += 1\n",
    "\n",
    "            x = s_neg\n",
    "            x[f] -= w[f]\n",
    "\n",
    "            # search other features\n",
    "            for i in range(n):\n",
    "                if i == f:\n",
    "                    continue\n",
    "\n",
    "                u = np.zeros(n)\n",
    "                u[i] = 1.0/delta\n",
    "                test = self.query([x + u, x - u])\n",
    "                m += 2\n",
    "                if test[0] == test[1]:\n",
    "                    w[i] = 0.0\n",
    "                else:\n",
    "                    step = 1\n",
    "                    x_0 = x.copy()\n",
    "                    x_1 = x.copy()\n",
    "\n",
    "                    assert self.query([x_0])[0] == c1\n",
    "                    #print 'x_0 = {}'.format(x_0)\n",
    "                    # exponential search\n",
    "                    while self.query([x_0]) != c0:\n",
    "                        #print 'query of {} = {}'.format(x_0, c1)\n",
    "\n",
    "                        if step > 0:\n",
    "                            step *= -1\n",
    "                        else:\n",
    "                            step *= -2\n",
    "\n",
    "                        if step != -1:\n",
    "                            x_1[i] = step/2 * x[i]\n",
    "                        x_0[i] = step * x[i]\n",
    "                        m += 1\n",
    "\n",
    "                    #print 'x_0 = {}'.format(x_0)\n",
    "                    #print 'x_1 = {}'.format(x_1)\n",
    "                    #print self.query([x_0, x_1])\n",
    "\n",
    "                    assert list(self.query([x_0, x_1])) == [c0, c1]\n",
    "\n",
    "                    mid = x_0.copy()\n",
    "\n",
    "                    # binary search\n",
    "                    while abs(x_0[i] - x_1[i]) >= eps/4:\n",
    "                        mid[i] = 0.5 * (x_0[i] + x_1[i])\n",
    "                        c = self.query([mid])\n",
    "                        if c == c1:\n",
    "                            x_1[i] = mid[i]\n",
    "                        else:\n",
    "                            x_0[i] = mid[i]\n",
    "                        m += 1\n",
    "\n",
    "                    w[i] = 1.0 / (x_1[i] - x[i])\n",
    "\n",
    "            intercept = -np.dot(x_1, w)\n",
    "\n",
    "            if m <= budget:\n",
    "                return np.array([w]), intercept, m\n",
    "\n",
    "        return np.zeros((1, n)), 0, m\n",
    "\n",
    "    def run(self, data, X_test, test_size=100000, random_seed=0,\n",
    "            alphas=[0.5, 1, 2, 5, 10, 20, 50, 100],\n",
    "            methods=[\"passive\", \"adapt-local\", \"adapt-oracle\"],\n",
    "            baseline=True):\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        #print (','.join(['%s'] * 9) \\\n",
    "              #% ('dataset', 'method', 'budget', 'mode', 'loss',\n",
    "                 #'loss_u', 'probas', 'probas_u', 'model l1'))\n",
    "\n",
    "        # number of unknown coefficients\n",
    "        k = len(self.get_classes())\n",
    "        n = self.num_features()\n",
    "        num_unknowns = (k - int(k == 2)) * (n + 1)\n",
    "        print('Target Classes : %d | num features : %d | num unknown　: %d '%(k,n,num_unknowns))\n",
    "\n",
    "        #num_unknowns = n+1\n",
    "        print(\"Generate query set......\")\n",
    "        X_test_u = self.gen_query_set(n, test_size, force_input_space=True)\n",
    "\n",
    "        print(\"Query set size: \", X_test_u.shape)\n",
    "        \n",
    "        if k == 2:\n",
    "            for alpha in alphas:\n",
    "                print(\"---------\")\n",
    "                print(\"current alpha: \", alpha)              \n",
    "                m = int(alpha * num_unknowns)\n",
    "                print(\"budget: \", m)\n",
    "                w_opt, int_opt = self.find_coeffs_bin(budget=m)\n",
    "                \n",
    "                print(\"weight opti: \", w_opt)\n",
    "                print(\"int_opti: \", int_opt)\n",
    "                \n",
    "                # compute the accuracy of the predictions\n",
    "                if X_test is not None:\n",
    "                    print(\"X_test shape: \",X_test.shape)\n",
    "                    acc = self.evaluate(w_opt, int_opt, X_test)\n",
    "                    l1 = self.evaluate_probas(w_opt, int_opt, X_test)\n",
    "                else:\n",
    "                    acc, l1 = [np.nan] * 2\n",
    "             \n",
    "                acc_u = self.evaluate(w_opt, int_opt, X_test_u)\n",
    "                print(\"Evaluate ----> acc_u: \", acc_u)\n",
    "                l1_u = self.evaluate_probas(w_opt, int_opt, X_test_u)\n",
    "                print(\"Evaluate Probas ----> l1_u: \", l1_u)\n",
    "                loss = self.evaluate_model(w_opt, int_opt)\n",
    "                print(\"Evaluate Model ----> loss:　\", loss)\n",
    "                \n",
    "                if X_test_u.shape[1] == 2 \\\n",
    "                        and callable(getattr(self, 'encode', None)):\n",
    "                    print (X_test_u)\n",
    "                    utils.plot_decision_boundary(\n",
    "                        lambda x: predict_classes(self.encode(x), w_opt,\n",
    "                                                  int_opt, self.get_classes()),\n",
    "                        X_test_u,\n",
    "                        self.query(X_test_u),\n",
    "                        bounds=[-1, 1, -1, 1]\n",
    "                    )\n",
    "\n",
    "                print ('%s,%s,%d,extr,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                      (data, 'binary', m, 1-acc, 1-acc_u,\n",
    "                       l1, l1_u, loss))\n",
    "                print(\"--------------------------------------------------------------------------\")\n",
    "                \n",
    "                \"\"\"\n",
    "                if baseline and not callable(getattr(self, 'encode', None)):\n",
    "                    w_base, int_base, m_base = self.lowd_meek(budget=m)\n",
    "\n",
    "                    # compute the accuracy of the predictions\n",
    "                    if X_test is not None:\n",
    "                        acc_base = self.evaluate(w_base, int_base, X_test)\n",
    "                        l1_base = self.evaluate_probas(w_base, int_base, X_test)\n",
    "                    else:\n",
    "                        acc_base, l1_base = [np.nan] * 2\n",
    "\n",
    "                    acc_u_base = self.evaluate(w_base, int_base, X_test_u)\n",
    "                    l1_u_base = self.evaluate_probas(w_base, int_base, X_test_u)\n",
    "                    loss_base = self.evaluate_model(w_base, int_base)\n",
    "\n",
    "                    print ('%s,%s,%d,base,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, 'lowd-meek', m,\n",
    "                           1-acc_base, 1-acc_u_base, l1_base, l1_u_base,\n",
    "                           loss_base))\n",
    "                \"\"\"\n",
    "\n",
    "        for alpha in alphas:\n",
    "            m = int(alpha * num_unknowns)\n",
    "            step = (m + 4)/5\n",
    "\n",
    "            for method in methods:\n",
    "\n",
    "                if m < 5:\n",
    "                    print ('%s,%s,%d,extr,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, method, m, 1.0, 1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "                    print ('%s,%s,%d,base,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, method, m, 1.0, 1.0, 1.0, 1.0, 1.0))\n",
    "                    continue\n",
    "\n",
    "                base_model = None\n",
    "                if method == \"passive\":\n",
    "                    print(\"passize budget : \",m);\n",
    "                    w_opt, int_opt, m = self.find_coeffs(m)\n",
    "               \n",
    "                    if baseline:\n",
    "                        base_model = self.find_coeffs(m, baseline=True)\n",
    "                        #print(\"base_model.w: \",base_model.w)\n",
    "                elif method == \"adapt-local\":\n",
    "                    w_opt, int_opt = self.find_coeffs_adaptive(step, m)\n",
    "\n",
    "                    if baseline:\n",
    "                        base_model = self.find_coeffs_adaptive(step, m,\n",
    "                                                               baseline=True)\n",
    "                elif method == \"adapt-oracle\":\n",
    "                    w_opt, int_opt, m = self.find_coeffs(m, adapt=True)\n",
    "\n",
    "                    if baseline:\n",
    "                        base_model = self.find_coeffs(m, baseline=True,\n",
    "                                                      adapt=True)\n",
    "\n",
    "                # compute the accuracy of the predictions\n",
    "                if X_test is not None:\n",
    "                    #print(\"X_test.shape:　\",X_test.shape)\n",
    "                    acc = self.evaluate(w_opt, int_opt, X_test,\n",
    "                                        base_model=base_model)\n",
    "                    print(\"acc: \",(acc))\n",
    "                    l1 = self.evaluate_probas(w_opt, int_opt, X_test,\n",
    "                                              base_model=base_model)\n",
    "                    print(\"l1: \",l1)\n",
    "                else:\n",
    "                    if baseline:\n",
    "                        acc = [np.nan] * 2\n",
    "                        l1 = [np.nan] * 2\n",
    "                    else:\n",
    "                        acc, l1 = np.nan, np.nan\n",
    "                        \n",
    "                #print(\"X_test_u.shape:　\",X_test.shape)\n",
    "                acc_u = self.evaluate(w_opt, int_opt, X_test_u,\n",
    "                                      base_model=base_model)\n",
    "\n",
    "                l1_u = self.evaluate_probas(w_opt, int_opt, X_test_u,\n",
    "                                            base_model=base_model)\n",
    "\n",
    "                loss = self.evaluate_model(w_opt, int_opt,\n",
    "                                           base_model=base_model)\n",
    "\n",
    "                if baseline:\n",
    "                    print ('%s,%s,%d,extr,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, method, m, 1-acc[0], 1-acc_u[0], l1[0],\n",
    "                           l1_u[0], loss[0]))\n",
    "                            \n",
    "                    print ('%s,%s,%d,base,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, method, m, 1-acc[1], 1-acc_u[1], l1[1],\n",
    "                           l1_u[1], loss[1]))\n",
    "                else:\n",
    "                    print ('%s,%s,%d,extr,%.2e,%.2e,%.2e,%.2e,%.2e' % \\\n",
    "                          (data, method, m, 1-acc, 1-acc_u, l1, l1_u, loss))\n",
    "\n",
    "                if X_test_u.shape[1] == 2 \\\n",
    "                        and callable(getattr(self, 'encode', None)):\n",
    "                    utils.plot_decision_boundary(\n",
    "                        lambda x: predict_classes(self.encode(x), w_opt,\n",
    "                                                  int_opt, self.get_classes()),\n",
    "                        X_test_u,\n",
    "                        self.query(X_test_u),\n",
    "                        bounds=[-1, 1, -1, 1]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
