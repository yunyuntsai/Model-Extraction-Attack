{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 7, 8, 12, 13]\n",
      "[0, 2, 4, 9, 10, 11]\n",
      "[['State-gov' 'Bachelors' 'Never-married' ... 2174 0 40]\n",
      " ['Self-emp-not-inc' 'Bachelors' 'Married-civ-spouse' ... 0 0 13]\n",
      " ['Private' 'HS-grad' 'Divorced' ... 0 0 40]\n",
      " ...\n",
      " ['Private' 'Bachelors' 'Married-civ-spouse' ... 0 0 50]\n",
      " ['Private' 'Bachelors' 'Divorced' ... 5455 0 40]\n",
      " ['Self-emp-inc' 'Bachelors' 'Married-civ-spouse' ... 0 0 60]]\n",
      "input features:  ['Workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex', 'native-country', 'income', 'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "feature_types:  ['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'NUMERIC', 'NUMERIC', 'NUMERIC', 'NUMERIC', 'NUMERIC', 'NUMERIC']\n",
      "ONE HOT ENCODING: True\n",
      "classes:  ['Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Black' 'White' 'Other']\n",
      "BINNING: True\n",
      "feature: 8 type: NUMERIC\n",
      "line search on feature age\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "feature: 9 type: NUMERIC\n",
      "line search on feature fnlwgt\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "feature: 10 type: NUMERIC\n",
      "line search on feature education-num\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "feature: 11 type: NUMERIC\n",
      "line search on feature capital-gain\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "ending line search with 0.0 (0.0 - 0.537109375)\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "ending line search with -0.001 (-0.537109375 - 0.0)\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "feature: 12 type: NUMERIC\n",
      "line search on feature capital-loss\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "ending line search with 0.0 (0.0 - 0.537109375)\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "ending line search with -0.001 (-0.537109375 - 0.0)\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "1 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "feature: 13 type: NUMERIC\n",
      "line search on feature hours-per-week\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "0 queries can be served from cache\n",
      "1 queries to be sent to AWS\n",
      "threshold:  {'capital-gain': [0.0, -0.001], 'fnlwgt': [], 'education-num': [], 'hours-per-week': [], 'age': [], 'capital-loss': [0.0, -0.001]}\n",
      "Threshold finding required 94 queries\n",
      "Threshold finding took 22 seconds\n",
      "age: [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "fnlwgt: [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "education-num: [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "capital-gain: [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "capital-loss: [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "hours-per-week: [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "Input features: 14: ['Workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex', 'native-country', 'income', 'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Feature Space: 113\n",
      "Target Classes : 5 | num features : 113 | num unknownã€€: 570 \n",
      "Generate query set......\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "features:  [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "query set size:  (20, 14)\n",
      "passize budget :  570\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "features:  [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "features:  [Bin(-1.1, -0.001), Bin(-0.001, 0.0), Bin(0.0, 1.1)]\n",
      "features:  [Bin(-1.1, 0), Bin(0, 1.1)]\n",
      "Current query count: 94\n",
      "Sending 570 queries to the oracle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 queries can be served from cache\n",
      "560 queries to be sent to AWS\n",
      "Batch of 570 queries took 124 seconds\n",
      "w0.shape (5, 114)\n",
      "finding solution of system of 570 equations with 570 unknowns with BFGS\n",
      "optimize logit BFGS\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 413.392155\n",
      "         Iterations: 100\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 102\n",
      "Current query count: 654\n",
      "Sending 570 queries to the oracle\n",
      "570 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "Batch of 570 queries took 0 seconds\n",
      "obtained train accuracy of 1.0\n",
      "opti ran for 1.98 s\n",
      "Current query count: 654\n",
      "Sending 20 queries to the oracle\n",
      "0 queries can be served from cache\n",
      "20 queries to be sent to AWS\n",
      "Batch of 20 queries took 4 seconds\n",
      "acc:  1.0\n",
      "Current query count: 674\n",
      "Sending 20 queries to the oracle\n",
      "20 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "Batch of 20 queries took 0 seconds\n",
      "l1:  0.11820465790959014\n",
      "Current query count: 674\n",
      "Sending 20 queries to the oracle\n",
      "0 queries can be served from cache\n",
      "20 queries to be sent to AWS\n",
      "Batch of 20 queries took 4 seconds\n",
      "Current query count: 694\n",
      "Sending 20 queries to the oracle\n",
      "20 queries can be served from cache\n",
      "0 queries to be sent to AWS\n",
      "Batch of 20 queries took 0 seconds\n",
      "aws-adult,passive,570,extr,0.00e+00,0.00e+00,1.18e-01,6.34e-06,nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import argparse\n",
    "import _pickle\n",
    "from copy import copy\n",
    "import decimal\n",
    "import re\n",
    "import timeit\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "%run utils.ipynb\n",
    "%run regression_stealer.ipynb\n",
    "%run models.ipynb\n",
    "\n",
    "class Bin(object):\n",
    "    def __init__(self, lower, upper, eps):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.eps = eps\n",
    "        self.repr = []\n",
    "\n",
    "    def contains(self, val):\n",
    "        if self.lower == self.upper == val:\n",
    "            return True\n",
    "        else:\n",
    "            return self.lower < val <= self.upper\n",
    "\n",
    "    def repr_val(self, size=1):\n",
    "        \"\"\"\n",
    "        if self.upper-self.lower < 2*self.eps:\n",
    "            return np.array([self.upper] * size)\n",
    "\n",
    "        return [0.5 * (self.lower + self.upper)] * size\n",
    "        \"\"\"\n",
    "        return np.array([self.upper] * size)\n",
    "\n",
    "    def unif_val(self, size=1):\n",
    "\n",
    "        if self.upper-self.lower < 2*self.eps:\n",
    "            return np.array([self.upper] * size)\n",
    "\n",
    "        #return [0.5 * (self.lower + self.upper)] * size\n",
    "        return np.random.uniform(self.lower, self.upper, size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Bin({}, {})'.format(self.lower, self.upper)\n",
    "\n",
    "\n",
    "def make_query(query, feature_name, val):\n",
    "    \"\"\"\n",
    "    Modify a given query by switching the value of one feature\n",
    "    \"\"\"\n",
    "    new_query = copy(query)\n",
    "    new_query[feature_name] = val\n",
    "\n",
    "    return new_query\n",
    "\n",
    "\n",
    "def eps_round(x, epsilon):\n",
    "    \"\"\"\n",
    "    Round a floating point value to the nearest multiple of eps\n",
    "    \"\"\"\n",
    "    return round(x / epsilon) * epsilon\n",
    "\n",
    "class AWSRegressionExtractor(RegressionExtractor):\n",
    "\n",
    "    def __init__(self, model_id, X, cat_idx, incomplete=True, eps=1e-3):\n",
    "        self.model_id = model_id\n",
    "        self.client = boto3.client('machinelearning')\n",
    "\n",
    "        self.model = self.client.get_ml_model(\n",
    "            MLModelId=models[model_id]['id'],\n",
    "            Verbose=True\n",
    "        )\n",
    "        \n",
    "        schema = self.model['Schema']\n",
    "\n",
    "        target_reg = '\"targetAttributeName\":\"(.*?)\"'\n",
    "        target = re.findall(target_reg, schema)[0]\n",
    "\n",
    "        feature_reg = '\"attributeName\":\"(.*?)\",\"attributeType\":\"(.*?)\"'\n",
    "        feature_info = re.findall(feature_reg, schema)\n",
    "\n",
    "        features = [(f, t) for (f, t) in feature_info if f != target]\n",
    "\n",
    "        cat_features = [(f, t) for (f, t) in features if t == \"CATEGORICAL\"]\n",
    "        numeric_features = [(f, t) for (f, t) in features if t == \"NUMERIC\"]\n",
    "        binary_features = [(f, t) for (f, t) in features if t == \"BINARY\"]\n",
    "        \n",
    "        self.input_features = [f for (f, t) in cat_features + numeric_features + binary_features]\n",
    "        self.feature_types = [t for (f, t) in cat_features + numeric_features + binary_features]\n",
    "        print(\"input features: \",self.input_features)\n",
    "        print(\"feature_types: \",self.feature_types)\n",
    "        self.one_hot = len(cat_features) > 0\n",
    "        print('ONE HOT ENCODING: {}'.format(self.one_hot))\n",
    "\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        for i in range(len(cat_idx)):\n",
    "            self.label_encoders[i] = LabelEncoder()\n",
    "            X[:, i] = self.label_encoders[i].fit_transform(X[:, i])\n",
    "\n",
    "        self.encoder = OneHotEncoder(categorical_features=range(len(cat_idx)),\n",
    "                                     sparse=False)\n",
    "        self.encoder.fit(X)\n",
    "\n",
    "        self.classes = np.array(models[model_id]['classes'])\n",
    "        print(\"classes: \",self.classes)\n",
    "        self.multinomial = len(self.classes) != 2\n",
    "\n",
    "        self.eps = eps\n",
    "        self.query_count = 0\n",
    "\n",
    "        self.query_cache = {}\n",
    "\n",
    "        \"\"\"\n",
    "        self.cache_file = 'aws_wrapper/cache/{}.pkl'.format(model_id)\n",
    "        try:\n",
    "            with open(self.cache_file, 'rb') as f:\n",
    "                self.query_cache = cPickle.load(f)\n",
    "        except IOError:\n",
    "            logging.info('Creating Query Cache')\n",
    "        \"\"\"\n",
    "\n",
    "        self.binning = '_QB_' in self.model['Recipe']\n",
    "        print('BINNING: {}'.format(self.binning))\n",
    "        if self.binning:\n",
    "            self.bins = self.find_bins()\n",
    "        else:\n",
    "            self.bins = {}\n",
    "\n",
    "        print('Input features: {}: {}'.format(self.num_input_features(),\n",
    "                                                     self.input_features))\n",
    "        print('Feature Space: {}'.format(self.num_features()))\n",
    "\n",
    "        self.incomplete = incomplete\n",
    "\n",
    "        RegressionExtractor.__init__(self)\n",
    "\n",
    "    def num_features(self):\n",
    "        if not (self.binning or self.one_hot):\n",
    "            return len(self.input_features)\n",
    "        else:\n",
    "            tot = sum([len(b) for b in self.bins.values()])\n",
    "            if self.one_hot:\n",
    "                tot += sum(self.encoder.n_values_)\n",
    "            return tot\n",
    "\n",
    "    def num_input_features(self):\n",
    "        return len(self.input_features)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "    def make_ls_pred(self, q):\n",
    "        \n",
    "        return self.aws_query(np.array([q]))[0]['predictedScores']\n",
    "\n",
    "    def line_search(self, query, feature, min_val, max_val, min_p, max_p):\n",
    "        \"\"\"\n",
    "        Perform a line search on a continuous feature to find all splitting\n",
    "        thresholds\n",
    "        \"\"\"\n",
    "        #print('\\tmin val :{} got {}'.format(min_val, min_p))\n",
    "        #print('\\tmax val : {} got {}'.format(max_val, max_p))\n",
    "        prec = -decimal.Decimal(str(self.eps)).as_tuple().exponent\n",
    "        #print('\\prec : {} '.format(prec))\n",
    "        def int_line_search(query, feature, min_mult, max_mult, min_p, max_p):\n",
    "\n",
    "            if min_p == max_p:\n",
    "                split = (min_mult + max_mult)/2\n",
    "\n",
    "                split_query = make_query(query, feature,\n",
    "                                         round(split * self.eps, prec))\n",
    "                #print(\"split_query : \",split_query)\n",
    "                split_p = self.make_ls_pred(split_query)\n",
    "                #print(\"split_p : \",split_p)\n",
    "                if split_p == min_p:\n",
    "\n",
    "                    return []\n",
    "\n",
    "            # if we've narrowed down the split to less than epsilon,\n",
    "            # call it a day\n",
    "            if (max_mult - min_mult) <= 1:\n",
    "                threshold = round(min_mult * self.eps, prec)\n",
    "\n",
    "                print('ending line search with {} ({} - {})'.format(\n",
    "                    threshold, min_mult, max_mult))\n",
    "\n",
    "                return [threshold]\n",
    "\n",
    "            # split at half way and check recursively on both sides\n",
    "            split = (min_mult + max_mult)/2\n",
    "            split_query = make_query(query, feature,\n",
    "                                     round(split * self.eps, prec))\n",
    "            split_p = self.make_ls_pred(split_query)\n",
    "\n",
    "            #print('\\tval {} got {}'.format(round(split*self.eps, prec),split_p)\n",
    "\n",
    "            return int_line_search(query, feature, split,\n",
    "                                   max_mult, split_p, max_p) + \\\n",
    "                   int_line_search(query, feature, min_mult,\n",
    "                                   split, min_p, split_p)\n",
    "\n",
    "        max_mult = int(round(max_val / self.eps))\n",
    "        min_mult = int(round(min_val / self.eps))\n",
    "        return int_line_search(query, feature, min_mult, max_mult, min_p, max_p)\n",
    "\n",
    "    def find_bins(self):\n",
    "        start_time = timeit.default_timer()\n",
    "        thresholds = {}\n",
    "\n",
    "        q0 = [None] * len(self.input_features)\n",
    "  \n",
    "        #for i in range(len(self.input_features)):\n",
    "         #    if self.feature_types[i] == 'NUMERIC':               \n",
    "          #      q0[i] = 0\n",
    "        #print(q0)       \n",
    "        for i, f in enumerate(self.input_features):\n",
    "            if self.feature_types[i] != 'NUMERIC':               \n",
    "                continue\n",
    "            \n",
    "            print(\"feature: %d type: %s\"%(i,self.feature_types[i]))\n",
    "            print('line search on feature {}'.format(f))\n",
    "            val_min = -1.1\n",
    "            q_min = make_query(q0, i, val_min)\n",
    "            p_min = self.make_ls_pred(q_min)\n",
    "        \n",
    "            val_max = 1.1\n",
    "            q_max = make_query(q0, i, val_max)\n",
    "            p_max = self.make_ls_pred(q_max)\n",
    "\n",
    "            t = self.line_search(q0, i, val_min, val_max, p_min, p_max)\n",
    "            thresholds[f] = t\n",
    "        print(\"threshold: \", thresholds)\n",
    "        end_time = timeit.default_timer()\n",
    "\n",
    "        print ('Threshold finding required %d queries' % self.query_count)\n",
    "        print ('Threshold finding took %d seconds' % (end_time - start_time))\n",
    "        #print(\"Threshold: \",thresholds)\n",
    "        bins = {}\n",
    "\n",
    "        for (f, t) in thresholds.items():\n",
    "            if len(t) == 0: \n",
    "                for j in range(0,1):\n",
    "                       t.append(0) \n",
    "            t.sort()\n",
    "            #print([Bin(-1.1, t[0], self.eps)])\n",
    "            bins[f] = [Bin(-1.1, t[0], self.eps)] + [Bin(t[i-1], t[i], self.eps) for i in range(1, len(t))] + [Bin(t[-1], 1.1, self.eps)]\n",
    "            \n",
    "        for f in self.input_features:\n",
    "            if f in bins:\n",
    "                print('{}: {}'.format(f, bins[f]))\n",
    "\n",
    "        return bins\n",
    "\n",
    "    def find_bin_index(self, v, f):\n",
    "        if v <= self.bins[f][0].lower:\n",
    "            return 0\n",
    "\n",
    "        if v > self.bins[f][-1].upper:\n",
    "            return len(self.bins[f])-1\n",
    "\n",
    "        for i in range(len(self.bins[f])):\n",
    "            if self.bins[f][i].contains(v):\n",
    "                return i\n",
    "        raise ValueError('{} not found for bins {}'.format(v, self.bins[f]))\n",
    "\n",
    "    def encode(self, X_input):\n",
    "        assert self.binning or self.one_hot\n",
    "        assert X_input.shape[1] == len(self.input_features)\n",
    "\n",
    "        X_input = X_input.copy()\n",
    "        X = []\n",
    "\n",
    "        for i in range(len(self.label_encoders)):\n",
    "            if isinstance(X_input[0, i], str):\n",
    "                X_input[:, i] = self.label_encoders[i].transform(X_input[:, i])\n",
    "\n",
    "        X_input = self.encoder.transform(X_input)\n",
    "\n",
    "        X_input_cat = X_input[:, :-len(self.bins)]\n",
    "        X_input_cont = X_input[:, -len(self.bins):]\n",
    "\n",
    "        numeric_features = [f for (f, t)\n",
    "                            in zip(self.input_features, self.feature_types)\n",
    "                            if t == \"NUMERIC\"]\n",
    "\n",
    "        for col, f in enumerate(numeric_features):\n",
    "            temp = np.zeros((len(X_input_cont), len(self.bins[f])))\n",
    "            idx = [self.find_bin_index(x, f) for x in X_input_cont[:, col]]\n",
    "            temp[np.arange(len(X_input)), idx] = 1\n",
    "            X.append(temp)\n",
    "\n",
    "        X = np.hstack(X)\n",
    "        X = np.hstack((X_input_cat, X))\n",
    "\n",
    "        assert X.shape[1] == self.num_features()\n",
    "        return X\n",
    "\n",
    "    def decode(self, x):\n",
    "        vals = []\n",
    "        idx = 0\n",
    "        for (i, f) in enumerate(self.input_features):\n",
    "            if self.feature_types[i] == \"CATEGORICAL\":\n",
    "                l = list(x[self.encoder.feature_indices_[i]:self.encoder.\n",
    "                         feature_indices_[i+1]])\n",
    "                assert sum(l) == 1 or sum(l) == 0\n",
    "\n",
    "                if sum(l) == 0:\n",
    "                    val = None\n",
    "                else:\n",
    "                    val = l.index(1)\n",
    "                    val = self.label_encoders[i].inverse_transform(val)\n",
    "\n",
    "                incr = self.encoder.n_values_[i]\n",
    "            else:\n",
    "                l = list(x[idx:idx+len(self.bins[f])])\n",
    "                assert sum(l) == 1 or sum(l) == 0\n",
    "\n",
    "                if sum(l) == 0:\n",
    "                    val = None\n",
    "                else:\n",
    "                    bin = self.bins[f][l.index(1)]\n",
    "                    val = bin.repr_val()[0]\n",
    "\n",
    "                incr = len(self.bins[f])\n",
    "            vals.append(val)\n",
    "            idx += incr\n",
    "        return vals\n",
    "\n",
    "    def random_input(self, test_size=1):\n",
    "\n",
    "        if not self.one_hot:\n",
    "            X = np.zeros((test_size, len(self.input_features)))\n",
    "        else:\n",
    "            X = np.empty((test_size, len(self.input_features)), dtype=object)\n",
    "\n",
    "        for i, f in enumerate(self.input_features):\n",
    "            if self.feature_types[i] == \"CATEGORICAL\":\n",
    "                # choose random values for the categorical features\n",
    "                X[:, i] = np.random.choice(self.label_encoders[i].classes_,\n",
    "                                           size=test_size)\n",
    "            else:\n",
    "                if self.binning:\n",
    "                    # choose uniformly random values for the continuous features\n",
    "                    print(\"features: \",self.bins[f])\n",
    "                    X[:, i] = [self.bins[f][j].unif_val()\n",
    "                               for j in np.random.choice(len(self.bins[f]),\n",
    "                                                         size=test_size)]\n",
    "                else:\n",
    "                    X[:, i] = utils.gen_query_set(1, test_size)[:, 0]\n",
    "        return X\n",
    "\n",
    "    def gen_query_set(self, n, test_size, force_input_space=False):\n",
    "        if force_input_space or len(self.input_features) == n:\n",
    "            X = self.random_input(test_size)\n",
    "            if self.binning:\n",
    "                r = -decimal.Decimal(str(self.eps)).as_tuple().exponent\n",
    "                for i, t in enumerate(self.feature_types):\n",
    "                    if t == \"NUMERIC\":\n",
    "                        X[:, i] = np.round(X[:, i].astype(np.float), r)\n",
    "            return X\n",
    "\n",
    "        if not self.incomplete:\n",
    "            rank = 0\n",
    "            temp = np.hstack((self.encode(self.random_input()), [[1]]))\n",
    "\n",
    "            counter = 0\n",
    "            while rank < min(test_size, self.num_features()+1) \\\n",
    "                    and counter < 10*self.num_features():\n",
    "\n",
    "                row = np.hstack((self.encode(self.random_input()), [[1]]))\n",
    "\n",
    "                new_rank = np.linalg.matrix_rank(np.vstack((temp, row)))\n",
    "                if new_rank > rank:\n",
    "                    rank = new_rank\n",
    "                    temp = np.vstack((temp, row))\n",
    "                counter += 1\n",
    "\n",
    "            temp = temp[:, :-1]\n",
    "        else:\n",
    "            temp = np.eye(self.num_features(), self.num_features())\n",
    "            temp = np.vstack((temp, np.zeros((1, self.num_features()))))\n",
    "\n",
    "        if len(temp) < test_size:\n",
    "            temp = np.vstack((temp, self.encode(\n",
    "                self.random_input(test_size - len(temp)))))\n",
    "\n",
    "        assert temp.shape == (test_size, self.num_features())\n",
    "        return temp\n",
    "\n",
    "    def prepare_query(self, x):\n",
    "        if len(x) != len(self.input_features):\n",
    "            x = self.decode(x)\n",
    "\n",
    "        feature_vector = dict(zip(self.input_features, x))\n",
    "        #print(feature_vector)\n",
    "        feature_vector = {k: str(v) for (k, v) in feature_vector.items()\n",
    "                          if v is not None}\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    def aws_query(self, X):\n",
    "        \n",
    "        if len(X) > 1:\n",
    "            print ('Current query count: %d' % self.query_count)\n",
    "            print ('Sending {} queries to the oracle'.format(len(X)))\n",
    "\n",
    "    \n",
    "        responses = np.array([None]*len(X))\n",
    "       \n",
    "        X = [self.prepare_query(x) for x in X]\n",
    "\n",
    "        X_cache = [(i, x) for (i, x) in enumerate(X)\n",
    "                   if hash(frozenset(x.items())) in self.query_cache]\n",
    "\n",
    "        X_query = [(i, x) for (i, x) in enumerate(X)\n",
    "                   if hash(frozenset(x.items())) not in self.query_cache]\n",
    "\n",
    "        print ('{} queries can be served from cache'.format(len(X_cache)))\n",
    "        print ('{} queries to be sent to AWS'.format(len(X_query)))\n",
    "\n",
    "        for (i, x) in X_cache:\n",
    "            qhash = hash(frozenset(x.items()))\n",
    "            responses[i] = self.query_cache[qhash]\n",
    "\n",
    "        \"\"\"\n",
    "        with open('temp.txt', 'w+') as f:\n",
    "            for (_, x) in X_query:\n",
    "                f.write('None,' + ','.join(x.values()) + '\\n')\n",
    "        \"\"\"\n",
    "\n",
    "        for (i, x) in X_query:\n",
    "                qhash = hash(frozenset(x.items()))\n",
    "                model = models['aws-adult-new']\n",
    "                model_id = model['id']\n",
    "                #print(\"model id: \", model_id)\n",
    "                #print(\"record: \",x)\n",
    "                response = self.client.predict(\n",
    "                    MLModelId= model_id,\n",
    "                    Record=x,\n",
    "                    PredictEndpoint=\n",
    "                    'https://realtime.machinelearning.us-east-1.amazonaws.com'\n",
    "                )['Prediction']\n",
    "\n",
    "                self.query_count += 1\n",
    "                self.query_cache[qhash] = response\n",
    "                responses[i] = response\n",
    "        #print(\"Responses: \",responses)\n",
    "        return responses\n",
    "\n",
    "    def query_probas(self, X):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        responses = self.aws_query(X)\n",
    "        #print(\"responsesssss: \",responses)\n",
    "        probas = np.zeros((len(X), len(self.get_classes())))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            scores = responses[i]['predictedScores']\n",
    "            if len(self.get_classes()) == 2:\n",
    "                class_probas = [1-scores.values()[0], scores.values()[0]]\n",
    "\n",
    "            else:\n",
    "                class_probas = [scores[c] for c in self.get_classes()]\n",
    "            probas[i] = class_probas\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "\n",
    "        if len(X) > 1:\n",
    "            print( 'Batch of %d queries took %d seconds' \\\n",
    "                  % (len(X), end_time-start_time))\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def query(self, X):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        responses = self.aws_query(X)\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            label = str(responses[i]['predictedLabel'])\n",
    "            labels.append(label)\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "\n",
    "        if len(X) > 1:\n",
    "            print ('Batch of %d queries took %d seconds' \\\n",
    "                  % (len(X), end_time-start_time))\n",
    "\n",
    "        return np.array(labels)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('data', type=str, help='a dataset')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "    parser.add_argument('--verbose', action='store_true')\n",
    "    parser.add_argument('--incomplete', dest='incomplete',\n",
    "                        action='store_true', help='allow incomplete queries')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dataset = args.data\n",
    "    seed = args.seed\n",
    "    incomplete = args.incomplete\n",
    "    verbose = args.verbose\n",
    "\n",
    "    if verbose:\n",
    "        level = logging.INFO\n",
    "\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(level)\n",
    "        ch = logging.StreamHandler(sys.stderr)\n",
    "        ch.setLevel(level)\n",
    "        formatter = logging.Formatter('%(message)s')\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    _, _, X, _, _ = prepare_data(\"adult\", onehot=False, labelEncode=False)\n",
    "\n",
    "    cat_idx = [i for i in range(len(X.columns))\n",
    "               if isinstance(X.iloc[0][i], str)]\n",
    "    print(cat_idx)\n",
    "    cont_idx = [i for i in range(X.shape[1])]\n",
    "\n",
    "    for i in cat_idx:\n",
    "        cont_idx.remove(i)\n",
    "    print(cont_idx)\n",
    "    X = X[cat_idx + cont_idx].values\n",
    "    print (X)\n",
    "    ext = AWSRegressionExtractor(\"aws-adult-new\", X.copy(), cat_idx,\n",
    "                                 incomplete=True)\n",
    "\n",
    "    try:\n",
    "        X_test = X[0:500]\n",
    "\n",
    "        if ext.binning:\n",
    "            r = -decimal.Decimal(str(ext.eps)).as_tuple().exponent\n",
    "            for i, t in enumerate(ext.feature_types):\n",
    "                if t == \"NUMERIC\":\n",
    "                    X_test[:, i] = np.round(X_test[:, i].astype(np.float), r)\n",
    "    except ValueError:\n",
    "        X_test = None\n",
    "\n",
    "    ext.run(\"aws-adult\", X_test, 500, random_seed=0,\n",
    "            alphas=[1], methods=['passive'], baseline=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
